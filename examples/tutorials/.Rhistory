fit
hist(logz, breaks = 80, probability = T)
curve(dnorm(x, fit$estimate[1], fit$estimate[2]), col = "red", lwd = 2, add = T)
curve(dnorm(x, mean(log(z$s)), sd(log(z$s))), col = 'blue', lwd = 2, add = T)
mean(log(z$s))
logz <- log(z$s[z$s > 0])
logz <- log(z$s[z$s > 0])
fit <- MASS::fitdistr(logz, densfun = "normal")
fit
hist(logz, breaks = 80, probability = T)
curve(dnorm(x, fit$estimate[1], fit$estimate[2]), col = "red", lwd = 2, add = T)
curve(dnorm(x, mean(logz), sd(logz)), col = 'blue', lwd = 2, add = T)
hist(logz, breaks = 80, probability = T)
curve(dnorm(x, fit$estimate[1], fit$estimate[2]), col = "red", lwd = 2, add = T)
#curve(dnorm(x, mean(logz), sd(logz)), col = 'blue', lwd = 2, add = T)
hist(logz, breaks = 80, probability = T)
curve(dnorm(x, fit$estimate[1], fit$estimate[2]), col = "red", lwd = 2, add = T)
curve(dnorm(x, mean(logz), sd(logz)), col = 'blue', lwd = 2, add = T)
logz <- log(z$s[z$s > 0])
fit <- MASS::fitdistr(logz, densfun = "gamma")
logz <- log(z$s[z$s > 1])
fit <- MASS::fitdistr(logz, densfun = "gamma")
fit
hist(logz, breaks = 80, probability = T)
curve(dgamma(x, fit$estimate[1], fit$estimate[2]), col = "red", lwd = 2, add = T)
curve(dnorm(x, mean(logz), sd(logz)), col = 'blue', lwd = 2, add = T)
logz <- log(z$s[z$s > 0])
fit <- MASS::fitdistr(logz, densfun = "normal")
fit
hist(logz, breaks = 80, probability = T)
curve(dnorm(x, fit$estimate[1], fit$estimate[2]), col = "red", lwd = 2, add = T)
curve(dnorm(x, mean(logz), sd(logz)), col = 'blue', lwd = 2, add = T)
logz1 <- log(z$s[z$s > 1])
fit <- MASS::fitdistr(logz1, densfun = "gamma")
fit
hist(logz, breaks = 80, probability = T)
curve(dgamma(x, fit$estimate[1], fit$estimate[2]), col = "red", lwd = 2, add = T)
curve(dnorm(x, mean(logz), sd(logz)), col = 'blue', lwd = 2, add = T)
install.packages(c("apcluster", "arules", "bnlearn", "chron", "coin", "doParallel", "DT", "forecast", "ggplot2", "h2o", "hms", "httr", "knitr", "lava", "libcoin", "lpSolve", "markdown", "modelr", "pkgbuild", "quantreg", "rgeos", "rjags", "rmarkdown", "rqdatatable", "rquery", "seriation", "sys", "tensorflow", "tinytex", "visNetwork", "whisker", "xfun", "xgboost", "xml2"))
library(readr)
library(caret)
library(dplyr)
library(xgboost)
library(xrf)
library(PRROC)
library(ROSE)
# library(doMC)
# registerDoMC(cores = detectCores(logical = FALSE))
#datafile <- '../data/small.csv'
datafile <- '../data/creditcard.csv'
# df <- read_csv(datafile, n_max = 40000)
df <- read_csv(datafile)
#df$Class <- as.factor(ifelse(df$Class == 1, 'Y', 'N'))
print(names(df))
set.seed(42)
inTrain <- createDataPartition(df$Class, p = 0.7, list = FALSE)
# train <- ovun.sample(Class ~ ., df[inTrain,],
#                      method = 'over', N = 2 * length(inTrain), seed = 42)$data
train <- df[inTrain,]
test <- df[-inTrain,]
X <- train %>% select(-Class)
y <- train$Class
prop <- 1 / mean(y)
weights <- ifelse(y, prop, 1)
n_folds = 3
X_test <- test %>% select(-Class)
y_test <- test$Class
xgb_control = list(max_depth = 3,
base_score = 0.00015,
colsample_bytree = 0.34,
gamma = 2,
learning_rate = 0.46,
reg_alpha = 0.38,
reg_lambda = 7,
subsample = 0.4,
objective = 'binary:logistic',
eval_metric = 'aucpr',
seed = 42)
# xgb_control = list(max_depth = 3,
#                    base_score = 0.0002,
#                    colsample_bytree = 0.8,
#                    gamma = 2,
#                    learning_rate = 0.65,
#                    reg_alpha = 4,
#                    reg_lambda = 16,
#                    subsample = 0.85,
#                    objective = 'binary:logistic',
#                    eval_metric = 'aucpr',
#                    seed = 43)
xgb_control = list(max_depth = 3,
base_score = 0.5,
colsample_bytree = 0.4,
gamma = 2,
learning_rate = 0.4,
reg_alpha = 2,
reg_lambda = 2,
subsample = 0.5,
objective = 'binary:logistic',
eval_metric = 'aucpr',
# tree_method = 'gpu_hist',
seed = 1)
dtrain <- xgb.DMatrix(as.matrix(X),
label = y)
dtest <- xgb.DMatrix(as.matrix(X_test), label = y_test)
rounds <- c(81, 61, 41)
xgb <- NULL
for (depth in 1:3) {
xgb_control$max_depth <- depth
print(xgb_control)
if (is.null(xgb))
xgb <- xgb.train(xgb_control,
dtrain,
watchlist = list(train = dtrain, eval = dtest),
nrounds = rounds[[depth]],
print_every_n = 20)
else
xgb <- xgb.train(xgb_control,
dtrain,
xgb_model = xgb,
watchlist = list(train = dtrain, eval = dtest),
nrounds = rounds[[depth]],
print_every_n = 20)
}
y_test_xgb <- predict(xgb, as.matrix(X_test), type = 'prob')
pr_xgb <- pr.curve(y_test_xgb[y_test == 1], y_test_xgb[y_test == 0], curve = TRUE)
pr_xgb # 0.826
plot(pr_xgb)
set.seed(37)
glm_control <- list(type.measure = 'auc',
nfolds = n_folds,
pmax = 90,
weights = weights ^ 0.4)
system.time({
clf <- xrf(Class ~ .,
data = train,
prefit_xgb = xgb,
glm_control = glm_control,
family = 'binomial',
sparse = FALSE)
})
# not working well with weights ^0.25 -> move to 0.35
y_test_xrf <- predict(clf, df[-inTrain,], type = 'response')
pr_xrf <- pr.curve(y_test_xrf[y_test == 1], y_test_xrf[y_test == 0], curve = TRUE)
pr_xrf # 0.72
clf.xrf <- clf
save(clf.xrf, file = '../models/clf.xrf.0.4.detph123.rda')
plot(pr_xrf)
knitr::kable(coef(clf) %>% filter(coefficient_lambda.min != 0))
library(shiny); runApp('gitlab/vast08/shiny/dashnet.R')
install.packages(c("cdata", "curl", "devtools", "fuzzyjoin", "httpuv", "matrixStats", "RcppArmadillo", "recipes", "rmgarch", "RODBC", "rqdatatable", "rquery", "StanHeaders", "tidyr", "vtreat", "WVPlots", "zip"))
library(caret)
library(titanic)
library(catboost)
set.seed(12345)
data <- as.data.frame(as.matrix(titanic_train), stringsAsFactors = TRUE)
drop_columns = c("PassengerId", "Survived", "Name", "Ticket", "Cabin")
x <- data[,!(names(data) %in% drop_columns)]
y <- data[,c("Survived")]
fit_control <- trainControl(method = "cv",
number = 4,
classProbs = TRUE)
grid <- expand.grid(depth = c(4, 6, 8),
learning_rate = 0.1,
iterations = 100,
l2_leaf_reg = 1e-3,
rsm = 0.95,
border_count = 64)
report <- train(x, as.factor(make.names(y)),
method = catboost.caret,
logging_level = 'Verbose', preProc = NULL,
tuneGrid = grid, trControl = fit_control)
print(report)
importance <- varImp(report, scale = FALSE)
print(importance)
library(xrf)
print(names(train))
```{r}
train <- read_csv("dfTrain.csv")
library(readr)
library(caret)
library(dplyr)
library(xgboost)
library(xrf)
library(PRROC)
library(ROSE)
train <- read_csv("dfTrain.csv")
test <- read_csv("dfTest.csv")
print(names(train))
train <- read_csv("dfTrain.csv")
test <- read_csv("dfTest.csv")
print(names(train))
y_test <- test$Class
y_test <- test$Class
train <- read_csv("dfTrain.csv")
test <- read_csv("dfTest.csv")
X <- train %>% select(-Class)
train <- read_csv("dfTrain.csv")
test <- read_csv("dfTest.csv")
X <- train %>% select(-Class)
y <- train$Class
prop <- 1 / mean(y)
weights <- ifelse(y, prop, 1)
n_folds = 3
X_test <- test %>% select(-Class)
y_test <- test$Class
rounds <- c(40, 20)
xgb_control = list(max_depth = 3,
objective = 'binary:logistic',
eval_metric = 'aucpr',
seed = 42)
dtrain <- xgb.DMatrix(as.matrix(X),
label = y)
dtest <- xgb.DMatrix(as.matrix(X_test),
label = y_test)
rounds <- c(40, 20)
xgb <- NULL
set.seed(xgb_control$seed)
for (depth in 1:length(rounds)) {
xgb_control$max_depth <- depth
print(xgb_control$max_depth)
if (is.null(xgb))
xgb <- xgb.train(xgb_control,
dtrain,
watchlist = list(train = dtrain, eval = dtest),
nrounds = rounds[[depth]],
print_every_n = 20)
else
xgb <- xgb.train(xgb_control,
dtrain,
xgb_model = xgb,
watchlist = list(train = dtrain, eval = dtest),
nrounds = rounds[[depth]],
print_every_n = 20)
}
xgb_control = list(max_depth = 3,
objective = 'binary:logistic',
eval_metric = 'aucpr',
seed = 42)
dtrain <- xgb.DMatrix(as.matrix(X),
label = y)
dtest <- xgb.DMatrix(as.matrix(X_test),
label = y_test)
rounds <- c(40, 20, 20)
xgb <- NULL
set.seed(xgb_control$seed)
for (depth in 1:length(rounds)) {
xgb_control$max_depth <- depth
print(xgb_control$max_depth)
if (is.null(xgb))
xgb <- xgb.train(xgb_control,
dtrain,
watchlist = list(train = dtrain, eval = dtest),
nrounds = rounds[[depth]],
print_every_n = 20)
else
xgb <- xgb.train(xgb_control,
dtrain,
xgb_model = xgb,
watchlist = list(train = dtrain, eval = dtest),
nrounds = rounds[[depth]],
print_every_n = 20)
}
xgb_control = list(max_depth = 3,
objective = 'binary:logistic',
eval_metric = 'aucpr',
learning_rate = 0.2,
subsample = 0.6,
colsample_bytree = 0.5,
seed = 42)
dtrain <- xgb.DMatrix(as.matrix(X),
label = y)
dtest <- xgb.DMatrix(as.matrix(X_test),
label = y_test)
rounds <- c(40, 20)
xgb <- NULL
set.seed(xgb_control$seed)
for (depth in 1:length(rounds)) {
xgb_control$max_depth <- depth
print(xgb_control$max_depth)
if (is.null(xgb))
xgb <- xgb.train(xgb_control,
dtrain,
watchlist = list(train = dtrain, eval = dtest),
nrounds = rounds[[depth]],
print_every_n = 20)
else
xgb <- xgb.train(xgb_control,
dtrain,
xgb_model = xgb,
watchlist = list(train = dtrain, eval = dtest),
nrounds = rounds[[depth]],
print_every_n = 20)
}
xgb_control = list(max_depth = 3,
objective = 'binary:logistic',
eval_metric = 'aucpr',
learning_rate = 0.2,
subsample = 0.6,
colsample_bytree = 0.5,
gamma = 2,
seed = 42)
dtrain <- xgb.DMatrix(as.matrix(X),
label = y)
dtest <- xgb.DMatrix(as.matrix(X_test),
label = y_test)
rounds <- c(40, 20)
xgb <- NULL
set.seed(xgb_control$seed)
for (depth in 1:length(rounds)) {
xgb_control$max_depth <- depth
print(xgb_control$max_depth)
if (is.null(xgb))
xgb <- xgb.train(xgb_control,
dtrain,
watchlist = list(train = dtrain, eval = dtest),
nrounds = rounds[[depth]],
print_every_n = 20)
else
xgb <- xgb.train(xgb_control,
dtrain,
xgb_model = xgb,
watchlist = list(train = dtrain, eval = dtest),
nrounds = rounds[[depth]],
print_every_n = 20)
}
setwd("C:/home/gitlab/fraudexplo/train")
library(readr)
library(caret)
library(dplyr)
library(mgcv)
library(PRROC)
auprcSummary <- function(data, lev = NULL, model = NULL){
index_class2 <- data$obs == "Y"
index_class1 <- data$obs == "N"
the_curve <- pr.curve(data$Y[index_class2],
data$Y[index_class1],
curve = FALSE)
out <- the_curve$auc.integral
names(out) <- "AUCPR"
out
}
datafile <- 'dfTrain.csv'
df <- read_csv(datafile) %>% as.data.frame
library(readr)
library(caret)
library(dplyr)
library(mgcv)
library(PRROC)
auprcSummary <- function(data, lev = NULL, model = NULL){
index_class2 <- data$obs == "Y"
index_class1 <- data$obs == "N"
the_curve <- pr.curve(data$Y[index_class2],
data$Y[index_class1],
curve = FALSE)
out <- the_curve$auc.integral
names(out) <- "AUCPR"
out
}
datafile <- 'dfTrain.csv'
df <- read_csv(datafile) %>% as.data.frame
df$Class <- as.factor(ifelse(df$Class == 1, 'Y', 'N'))
print(names(df))
fitControl <- trainControl(method = "cv",
number = 5,
summaryFunction = auprcSummary,
classProbs = TRUE,
search = 'random')
set.seed(42)
system.time(
model <- train(factor(Class)~.,
data = df,
method = "xgbTree",
nthread = 2,
trControl = fitControl,
verbose = TRUE,
allowParallel = FALSE,
tuneLength = 3)
)
df$Class
fitControl <- trainControl(method = "cv",
number = 5,
summaryFunction = auprcSummary,
classProbs = TRUE,
search = 'random')
set.seed(42)
system.time(
model <- train(Class~.,
data = df,
method = "xgbTree",
nthread = 2,
trControl = fitControl,
verbose = TRUE,
allowParallel = FALSE,
tuneLength = 3)
)
df %>% View
summary(df)
fitControl <- trainControl(method = "cv",
number = 5,
summaryFunction = auprcSummary,
classProbs = TRUE,
search = 'random')
set.seed(42)
system.time(
model <- train(Class~.,
data = df,
method = "xgbTree",
nthread = 2,
trControl = fitControl,
verbose = TRUE,
allowParallel = FALSE,
tuneLength = 3,
na.action = na.exclude)
)
model
fitControl <- trainControl(method = "cv",
number = 5,
summaryFunction = auprcSummary,
classProbs = TRUE,
search = 'random')
set.seed(42)
system.time(
model <- train(Class~.,
data = df,
method = "xgbTree",
nthread = 2,
trControl = fitControl,
verbose = TRUE,
allowParallel = FALSE,
tuneLength = 30,
na.action = na.exclude)
)
library(readr)
library(caret)
library(dplyr)
library(mgcv)
library(PRROC)
auprcSummary <- function(data, lev = NULL, model = NULL){
index_class2 <- data$obs == "Y"
index_class1 <- data$obs == "N"
the_curve <- pr.curve(data$Y[index_class2],
data$Y[index_class1],
curve = FALSE)
out <- the_curve$auc.integral
names(out) <- "AUCPR"
out
}
datafile <- 'dfTrain.csv'
df <- read_csv(datafile) %>% as.data.frame
df$Class <- as.factor(ifelse(df$Class == 1, 'Y', 'N'))
print(names(df))
fitControl <- trainControl(method = "cv",
number = 5,
summaryFunction = auprcSummary,
classProbs = TRUE,
search = 'random')
set.seed(42)
system.time(
model <- train(Class~.,
data = df,
method = "xgbTree",
nthread = 2,
trControl = fitControl,
verbose = TRUE,
allowParallel = FALSE,
tuneLength = 30,
na.action = na.exclude)
)
model
dim(df)
library(Hmisc)
print(dim(na.omit(df)))
impute_df <- function(df, fun = median) {
for (f in colnames(df))
df[f] = impute(data.frame(df)[f], fun = median)
df
}
df <- impute_df(df)
print(dim(na.omit(df)))
fitControl <- trainControl(method = "cv",
number = 5,
summaryFunction = auprcSummary,
classProbs = TRUE,
search = 'random')
set.seed(42)
system.time(
model <- train(Class~.,
data = df,
method = "xgbTree",
nthread = 2,
trControl = fitControl,
verbose = TRUE,
allowParallel = FALSE,
tuneLength = 30,
na.action = na.exclude)
)
model
model$results %>% arange(desc(AUCPR)) %>% head(20)
model$results %>% arrange(desc(AUCPR)) %>% head(20)
model$results %>% arrange(desc(AUCPR)) %>% head(20)
model$results
model$results %>% arrange(desc(AUCPR)) %>% head(20)
model$results %>% arrange(desc(AUCPR)) %>% head(20)
model$results %>% arrange(desc(AUCPR)
)
model$results %>% arrange(desc(AUCPR))
arrange
?arrange
model$results %>% dbplyr::arrange(desc(AUCPR))
desc
?dbplyr
?arrange
